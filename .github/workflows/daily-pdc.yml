name: Daily PDC

on:
  # Manual trigger from Actions tab
  workflow_dispatch:

  # Nightly run at 03:00 UTC (adjust to taste)
  schedule:
    - cron: '0 3 * * *'

  # Cross-repo trigger from Lab/Service/OS via repository_dispatch
  repository_dispatch:
    types: [pdc-compose]

# Needed for tag push and Pages publishing
permissions:
  contents: write

# Configure upstream repos and artifact names here
env:
  LAB_REPO: balaji-kadri/lab-workflow
  LAB_WORKFLOW: build.yml
  LAB_ARTIFACT: lab-artifact

  SERVICE_REPO: balaji-kadri/service-workflow
  SERVICE_WORKFLOW: build.yml
  SERVICE_ARTIFACT: service-artifact

  OS_REPO: balaji-kadri/os-workflow
  OS_WORKFLOW: build.yml
  OS_ARTIFACT: os-artifact

  PERF_BUDGET: "0.10"   # perf gate budget used by run_perf.py

concurrency:
  group: daily-pdc-${{ github.ref }}
  cancel-in-progress: true

jobs:
  compose:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout PDC repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Prepare inputs folder
        run: mkdir -p inputs

      # --- LAB: first try named download; if it fails, download ALL and normalize ---
      - name: Lab (try named artifact)
        uses: dawidd6/action-download-artifact@v2
        continue-on-error: true
        with:
          github_token: ${{ secrets.PDC_DISPATCH_TOKEN }}
          repo: ${{ env.LAB_REPO }}
          workflow: ${{ env.LAB_WORKFLOW }}
          name: ${{ env.LAB_ARTIFACT }}
          path: inputs/lab_named

      - name: Lab (fallback: download all)
        if: ${{ failure() }}
        uses: dawidd6/action-download-artifact@v2
        with:
          github_token: ${{ secrets.PDC_DISPATCH_TOKEN }}
          repo: ${{ env.LAB_REPO }}
          workflow: ${{ env.LAB_WORKFLOW }}
          path: inputs/lab_all

      - name: Normalize Lab
        shell: bash
        run: |
          set -euo pipefail
          # Prefer named folder, else all
          f="$(find inputs/lab_named -maxdepth 3 -type f -name 'lab.json' | head -n1 || true)"
          [ -z "$f" ] && f="$(find inputs/lab_all -maxdepth 3 -type f -name 'lab.json' | head -n1 || true)"
          if [ -z "$f" ]; then echo "❌ lab.json not found in Lab artifacts"; ls -R inputs || true; exit 2; fi
          cp "$f" inputs/lab.json

      # --- SERVICE: same tolerant pattern ---
      - name: Service (try named artifact)
        uses: dawidd6/action-download-artifact@v2
        continue-on-error: true
        with:
          github_token: ${{ secrets.PDC_DISPATCH_TOKEN }}
          repo: ${{ env.SERVICE_REPO }}
          workflow: ${{ env.SERVICE_WORKFLOW }}
          name: ${{ env.SERVICE_ARTIFACT }}
          path: inputs/service_named

      - name: Service (fallback: download all)
        if: ${{ failure() }}
        uses: dawidd6/action-download-artifact@v2
        with:
          github_token: ${{ secrets.PDC_DISPATCH_TOKEN }}
          repo: ${{ env.SERVICE_REPO }}
          workflow: ${{ env.SERVICE_WORKFLOW }}
          path: inputs/service_all

      - name: Normalize Service
        shell: bash
        run: |
          set -euo pipefail
          f="$(find inputs/service_named -maxdepth 3 -type f -name 'service.json' | head -n1 || true)"
          [ -z "$f" ] && f="$(find inputs/service_all -maxdepth 3 -type f -name 'service.json' | head -n1 || true)"
          if [ -z "$f" ]; then echo "❌ service.json not found in Service artifacts"; ls -R inputs || true; exit 2; fi
          cp "$f" inputs/service.json

      # --- OS: same tolerant pattern ---
      - name: OS (try named artifact)
        uses: dawidd6/action-download-artifact@v2
        continue-on-error: true
        with:
          github_token: ${{ secrets.PDC_DISPATCH_TOKEN }}
          repo: ${{ env.OS_REPO }}
          workflow: ${{ env.OS_WORKFLOW }}
          name: ${{ env.OS_ARTIFACT }}
          path: inputs/os_named

      - name: OS (fallback: download all)
        if: ${{ failure() }}
        uses: dawidd6/action-download-artifact@v2
        with:
          github_token: ${{ secrets.PDC_DISPATCH_TOKEN }}
          repo: ${{ env.OS_REPO }}
          workflow: ${{ env.OS_WORKFLOW }}
          path: inputs/os_all

      - name: Normalize OS
        shell: bash
        run: |
          set -euo pipefail
          f="$(find inputs/os_named -maxdepth 3 -type f -name 'os.json' | head -n1 || true)"
          [ -z "$f" ] && f="$(find inputs/os_all -maxdepth 3 -type f -name 'os.json' | head -n1 || true)"
          if [ -z "$f" ]; then echo "❌ os.json not found in OS artifacts"; ls -R inputs || true; exit 2; fi
          cp "$f" inputs/os.json

      - name: Check final inputs
        run: |
          echo "Inputs:"
          ls -la inputs
          for f in inputs/lab.json inputs/service.json inputs/os.json; do
            [ -f "$f" ] || { echo "❌ Missing $f"; exit 2; }
          done

      - name: Compose PDC
        run: |
          python3 scripts/compose_pdc.py \
            --lab inputs/lab.json \
            --service inputs/service.json \
            --os inputs/os.json

      - name: Upload composed artifact
        uses: actions/upload-artifact@v4
        with:
          name: pdc-composed
          path: dist/pdc.json

  test_and_gate:
    runs-on: ubuntu-latest
    needs: compose
    steps:
      - name: Checkout PDC repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Generate test reports
        run: |
          python3 scripts/run_e2e.py --report reports/e2e.json
          python3 scripts/run_perf.py --budget "${{ env.PERF_BUDGET }}" --report reports/perf.json
          python3 scripts/run_security.py --report reports/security.json
          python3 scripts/run_coverage.py --report reports/coverage.json

      - name: Verify reports exist
        run: |
          ls -la reports || { echo "reports/ missing"; exit 1; }
          for f in coverage.json e2e.json perf.json security.json; do
            [ -f "reports/$f" ] || { echo "❌ Missing reports/$f"; exit 2; }
          done
          echo "✅ All reports present."

      - name: Gate on results (reject release on policy violation)
        run: |
          python3 scripts/gate.py \
            --coverage reports/coverage.json \
            --e2e reports/e2e.json \
            --perf reports/perf.json \
            --security reports/security.json \
            --policy policies/pdc_policy.json

      - name: Upload test reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: pdc-test-reports
          path: reports

  release:
    runs-on: ubuntu-latest
    needs: test_and_gate
    if: ${{ needs.test_and_gate.result == 'success' }}
    steps:
      - name: Checkout PDC repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Download test reports
        uses: actions/download-artifact@v4
        with:
          name: pdc-test-reports
          path: reports

      - name: Configure Git (for tagging)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

      - name: Tag Release
        run: |
          TAG="pdc-$(date +'%Y.%m.%d')-build${{ github.run_number }}"
          echo "TAG=$TAG" >> $GITHUB_ENV
          git tag -a "$TAG" -m "Daily PDC release"
          git push origin "$TAG"
          echo "✅ Pushed tag: $TAG"

      - name: Emit dashboard payload (hardened script tolerates missing reports)
        run: |
          python3 scripts/emit_dashboard.py \
            --tag "$TAG" \
            --commit "${{ github.sha }}" \
            --reports reports \
            --out dashboard/payload.json

      - name: Upload dashboard payload
        uses: actions/upload-artifact@v4
        with:
          name: pdc-dashboard-payload
          path: dashboard/payload.json

      # --- GitHub Pages publishing (gh-pages branch) ---
      - name: Prepare dashboard workspace
        run: |
          rm -rf dashboard-work && mkdir -p dashboard-work
          # Copy your site assets (commit these in dashboard-site-src/)
          cp -r dashboard-site-src/* dashboard-work/
          # Copy latest payload
          cp dashboard/payload.json dashboard-work/payload.json

      - name: Bring in existing history (if any)
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: _ghpages

      - name: Merge history.json (append latest payload, cap to 200 entries)
        run: |
          python - << 'PY'
          import json, pathlib, time
          site = pathlib.Path('dashboard-work')
          ghp  = pathlib.Path('_ghpages')

          payload = json.loads((site/'payload.json').read_text(encoding='utf-8'))
          payload['timestamp'] = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

          hist = []
          if (ghp/'history.json').exists():
            try:
              hist = json.loads((ghp/'history.json').read_text(encoding='utf-8'))
              if not isinstance(hist, list): hist = []
            except Exception: hist = []

          tag = payload.get('build', {}).get('tag')
          hist = [e for e in hist if e.get('build', {}).get('tag') != tag]
          hist.append(payload)
          hist = sorted(hist, key=lambda e: e.get('timestamp',''), reverse=True)[:200]

          (site/'history.json').write_text(json.dumps(hist, indent=2), encoding='utf-8')
          PY

      - name: Deploy dashboard to GitHub Pages (gh-pages branch)
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: dashboard-work
          publish_branch: gh-pages
          keep_files: true
          commit_message: "Dashboard update: ${{ env.TAG }}"
``
